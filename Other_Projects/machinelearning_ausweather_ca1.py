# -*- coding: utf-8 -*-
"""MachineLearning_AusWeather_CA1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1m68UAKJlONVjmCd_pso46F4kw3fa3WLF
"""

# Data prepartion
# Loading libraries and functions
import pandas as pd
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from imblearn.over_sampling import SMOTE
from sklearn import linear_model
from sklearn.model_selection import GridSearchCV

# Loading the datatset
data=pd.read_csv('/content/weatherAUS.csv')

data

data['Date'] = pd.to_datetime(data['Date'], errors='coerce')
data['Date'] = data['Date'].dt.strftime('%Y%m%d').astype(int)

data['Date']

data['RainTomorrow'].unique()

data=data.dropna()

data.count()

data.info()

data['Location'].unique()

data['WindGustDir'].unique()

data['WindDir3pm'].unique()

data['WindDir9am'].unique()

data['Cloud9am'].unique()

data['Cloud3pm'].unique()

data['RainToday'].unique()

data['RainToday'] = data['RainToday'].map({'Yes':1,'No':0})

data['RainTomorrow'] = data['RainTomorrow'].map({'Yes':1,'No':0})

features=['Location','WindGustDir','WindDir3pm','WindDir9am','Cloud9am','Cloud3pm']
data=pd.get_dummies(data,columns=features)

data

Y=data['RainTomorrow']
Y

X=data.drop('RainTomorrow',axis=1)

Y.shape

X.shape

X_scaled=StandardScaler().fit_transform(X)

# Splitting the dataset into training and testing
X_train,X_test,Y_train,Y_test= train_test_split(X_scaled,Y, test_size=0.3,random_state=1)

# Balancing the dataset using SMOTE technique
X_train, Y_train=SMOTE(random_state=1).fit_resample(X_train,Y_train) # Balancing the class labels

"""# 1.Decision Tree Classification Model"""

from sklearn import tree

from sklearn.model_selection import GridSearchCV

from sklearn.model_selection import GridSearchCV
from sklearn import tree

# Define Decision Tree Classifier
dec_tree2 = tree.DecisionTreeClassifier(criterion='entropy')

# Define grid for max_depth parameter
depth = {'max_depth': [1, 2, 3, 4, 5, 10, 15, 20, 30, 40, 50, 100, 150, 200,300,400,500,1000]}

grid_search = GridSearchCV(estimator=dec_tree2, param_grid=depth, scoring='precision', cv=5)


# Fit the classifier to the training set
grid_search.fit(X_train, Y_train)

# Selecting the optimal parameter max_depth
best_depth = grid_search.best_params_
print(best_depth)

dec_tree_best_depth= tree.DecisionTreeClassifier(criterion='entropy',max_depth=10) # Decision Tree Classifier Building
dec_tree_best_depth.fit(X_train,Y_train) # Fitting decision tree classifier to the training set
Y_pred=dec_tree_best_depth.predict(X_test) # Predicted Y value on the test dataset

from sklearn import metrics

# Calculate accuracy
Accuracy = metrics.accuracy_score(Y_test, Y_pred)

# Calculate recall and precision with specified averaging for multiclass
Recall = metrics.recall_score(Y_test, Y_pred)
Precision = metrics.precision_score(Y_test, Y_pred)

print('Accuracy:', Accuracy)
print('Recall:', Recall)
print('Precision:', Precision)

dec_tree_best_depth_impt_feat= tree.DecisionTreeClassifier(criterion='entropy',max_depth=10)
dec_tree_best_depth_impt_feat.fit(X_train,Y_train)
Y_pred_impt_feat=dec_tree_best_depth_impt_feat.predict(X_test)
imp_features=pd.Series(dec_tree_best_depth_impt_feat.feature_importances_,index=list(X)).sort_values(ascending=False)
print(imp_features)

X3 = data[['Sunshine', 'Humidity3pm', 'Rainfall',
 'Pressure3pm', 'RainToday']]

# Normalization/ Standardisation of the new dataset with important features
X_scaled2=StandardScaler().fit_transform(X3)


# Dataset into training and testing
X_train,X_test,Y_train,Y_test= train_test_split(X_scaled2,Y, test_size=0.3,random_state=1)

# Balancing the dataset using SMOTE technique
X_train, Y_train=SMOTE(random_state=1).fit_resample(X_train,Y_train) # Balancing the class labels

# Define Decision Tree Classifier
dec_tree2 = tree.DecisionTreeClassifier(criterion='entropy')

# Define grid for max_depth parameter
depth = {'max_depth': [1, 2, 3, 4, 5, 10, 15, 20, 30, 40, 50, 100, 150, 200]}

grid_search = GridSearchCV(estimator=dec_tree2, param_grid=depth, scoring='precision', cv=5)


# Fit the classifier to the training set
grid_search.fit(X_train, Y_train)

# Selecting the optimal parameter max_depth
best_depth = grid_search.best_params_
print(best_depth)

dec_tree_best_depth= tree.DecisionTreeClassifier(criterion='entropy',max_depth=30) # Decision Tree Classifier Building
dec_tree_best_depth.fit(X_train,Y_train) # Fitting decision tree classifier to the training set
Y_pred=dec_tree_best_depth.predict(X_test) # Predicted Y value on the test dataset

from sklearn import metrics

# Calculate accuracy
Accuracy = metrics.accuracy_score(Y_test, Y_pred)

# Calculate recall and precision with specified averaging for multiclass
Recall = metrics.recall_score(Y_test, Y_pred)
Precision = metrics.precision_score(Y_test, Y_pred)

print('Accuracy:', Accuracy)
print('Recall:', Recall)
print('Precision:', Precision)

"""# 2. Random Forest"""

from sklearn import ensemble
RF= ensemble.RandomForestClassifier(n_estimators=50,criterion='entropy',max_features='log2',random_state=1)
RF.fit(X_train,Y_train) # fitting random forest to training data
Y_pred=RF.predict(X_test)

# Evaluating the random forest
from sklearn import metrics
Accuracy= metrics.accuracy_score(Y_test,Y_pred) # Calculating the accuracy of the model build
Recall= metrics.recall_score(Y_test,Y_pred) # Calculating the recall of the model build
Precision= metrics.precision_score(Y_test,Y_pred) # Calculating the precision of the model build
print('Accuracy:', Accuracy)
print('Recall:', Recall)
print('Precision:', Precision)

from sklearn.model_selection import GridSearchCV

# Selecting the optimal n_estimators parameter
RF1=ensemble.RandomForestClassifier(n_estimators=50,criterion='entropy',max_features='log2',random_state=1)
ntrees={'n_estimators': [50,100,150,180,200]} # Grid of n_estimators parameter length = 8

grid_search=GridSearchCV(estimator=RF1,param_grid=ntrees,scoring='precision',cv=5)

grid_search.fit(X_train,Y_train) # Fitting decision tree classifier to the training set
best_nestimator= grid_search.best_params_ # Selecting the optimal parameter n_estimators
print(best_nestimator)

# Refit the random forest on the optimal n_estimators parameter
RF2=ensemble.RandomForestClassifier(n_estimators=180,criterion='entropy',max_features='log2',random_state=1)
RF2.fit(X_train,Y_train) # fitting random forest to training data
Y_pred=RF2.predict(X_test)

# Evaluating the refitted random forest with the optimal n_estimator parameter
from sklearn import metrics
Accuracy= metrics.accuracy_score(Y_test,Y_pred) # Calculating the accuracy of the model build
Recall= metrics.recall_score(Y_test,Y_pred) # Calculating the recall of the model build
Precision= metrics.precision_score(Y_test,Y_pred) # Calculating the precision of the model build
print('Accuracy:', Accuracy)
print('Recall:', Recall)
print('Precision:', Precision)

"""# 3. Liner Regression"""

from plotly import figure_factory as ff
import plotly.graph_objects as go
cor = data.corr()
f = ff.create_annotated_heatmap(
    z=cor.values,
    x=list(cor.columns),
    y=list(cor.columns),
    annotation_text=cor.round(4).values,
    showscale=True
)
f.update_layout(
    width=6000,     # Increase width (in pixels)
    height=4000,    # Increase height (in pixels)
    title='Correlation Heatmap',
    xaxis=dict(tickangle=-45)  # Optional: angle x-axis labels if needed
)

f.show()

data1= data.drop(['Temp3pm'],axis=1)
data1.info()

X = data1.drop(['Temp9am'],axis=1)
Y = data1['Temp9am']

# Standardisation/normalisation of the dataset
X_st=StandardScaler().fit_transform(X)

# Linear Regression
LR=linear_model.SGDRegressor(random_state=1,penalty=None) # Linear regression without the regularisation
hyper_param={'eta0':[0.001,0.01,0.1,1],'max_iter':[1000,2000,3000,4000]}
grid_search=GridSearchCV(estimator=LR,param_grid=hyper_param,scoring='r2',cv=5) # R^2 tells how the good the linear model has fitted the dataset.
grid_search.fit(X_st,Y) # Fitting the linear model

# Optimal parameters
best_params= grid_search.best_params_
print(best_params)
best_result=grid_search.best_score_
print(best_result)
best_model=grid_search.best_estimator_
print('Beta_0:',best_model.intercept_)
#best_model.coef_
print(pd.DataFrame(zip(X.columns,best_model.coef_),columns=['Columns/features','Beta coefficients']))

"""## Regularisation"""

# Adding the penalty in the model
LR1=linear_model.SGDRegressor(random_state=1,penalty='elasticnet') # Linear regression without the regularisation
hyper_param={'eta0':[0.001,0.01,0.1,1],'max_iter':[1000,2000,3000,4000],'alpha':[0.001,0.01,0.1,1],'l1_ratio':[0.2,0.25,0.4,0.5,0.75,1]}
grid_search=GridSearchCV(estimator=LR1,param_grid=hyper_param,scoring='r2',cv=5) # R^2 tells how the good the linear model has fitted the dataset.
grid_search.fit(X_st,Y) # Fitting the linear model

# Optimal parameters
best_params= grid_search.best_params_
print(best_params)
best_result=grid_search.best_score_
print(best_result)
best_model=grid_search.best_estimator_
print('Beta_0:',best_model.intercept_)
#best_model.coef_
print(pd.DataFrame(zip(X.columns,best_model.coef_),columns=['Columns/features','Beta coefficients']))