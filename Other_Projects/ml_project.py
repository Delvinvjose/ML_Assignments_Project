# -*- coding: utf-8 -*-
"""ml project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1crO0G0Eu6tTgYCIXM6hKJ4N5_MrVvmPw
"""

import pandas as pd #needed
from pandas import read_csv, get_dummies, DataFrame,Series
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from imblearn.over_sampling import SMOTE
from sklearn import tree
from sklearn import metrics
from sklearn.model_selection import GridSearchCV

from sklearn.impute import SimpleImputer

data=read_csv('/content/Mental Health Dataset.csv')

data

data.isnull().sum()

data.info()

data.shape()

data.describe()

data['Gender'].unique()

data['Gender']=data['Gender'].map({'Female':1,'Male':0})

data['family_history'].unique()

data['family_history']=data['family_history'].map({'Yes':1,'No':0})

data['treatment'].unique()

data['treatment']=data['treatment'].map({'Yes':1,'No':0})

data['Coping_Struggles'].unique()

data['Coping_Struggles']=data['Coping_Struggles'].map({'Yes':1,'No':0})

multiclass_columns=['Country','Occupation','self_employed','Days_Indoors','Growing_Stress','Changes_Habits','Mental_Health_History','Mood_Swings','Work_Interest','Social_Weakness','mental_health_interview','care_options']
data2=pd.get_dummies(data,columns=multiclass_columns)

X=data2.drop(['treatment','Timestamp'],axis=1)
Y=data2['treatment']

X['self_employed_No'].isnull().sum()

scaler=StandardScaler()
X_scaled=scaler.fit_transform(X)

X_train, X_test, Y_train, Y_test = train_test_split(X_scaled, Y, test_size = 0.1, random_state = 42)

Y_train.value_counts()

dec_tree=tree.DecisionTreeClassifier(criterion='entropy',max_depth=5)

dec_tree.fit(X_train,Y_train)

Y_pred=dec_tree.predict(X_test)

Accuracy= metrics.accuracy_score(Y_test,Y_pred)
Recall= metrics.recall_score(Y_test,Y_pred)
Precision= metrics.precision_score(Y_test,Y_pred)
print('Accuracy:', Accuracy)
print('Recall:', Recall)
print('Precision:', Precision)

conf_mat= metrics.confusion_matrix(Y_test,Y_pred)
conf_mat

dec_tree2=tree.DecisionTreeClassifier(criterion='entropy')
depth={'max_depth':[5,10,15,20,25,30,35,40,45,46,47,48,49,50,51,52,53,54,55,60]}
grid_search=GridSearchCV(estimator=dec_tree2,param_grid=depth,scoring='precision',cv=5)
grid_search.fit(X_train,Y_train)
best_depth=grid_search.best_params_
print(best_depth)

dec_tree_best_depth=tree.DecisionTreeClassifier(criterion='entropy',max_depth=15)
dec_tree_best_depth.fit(X_train,Y_train)
Y_pred=dec_tree_best_depth.predict(X_test)

Accuracy= metrics.accuracy_score(Y_test,Y_pred)
Recall= metrics.recall_score(Y_test,Y_pred)
Precision= metrics.precision_score(Y_test,Y_pred)
print('Accuracy:', Accuracy)
print('Recall:', Recall)
print('Precision:', Precision)

dec_tree_best_depth_impt_feat= tree.DecisionTreeClassifier(criterion='entropy',max_depth=3)
dec_tree_best_depth_impt_feat.fit(X_train,Y_train)
Y_pred_impt_feat=dec_tree_best_depth_impt_feat.predict(X_test)
imp_features=Series(dec_tree_best_depth_impt_feat.feature_importances_,index=list(X)).sort_values(ascending=False)
print(imp_features)

# X2=data[['family_history','Gender']]
X2=data2[['family_history','care_options_Yes','Gender']]
X_scaled2=StandardScaler().fit_transform(X2)
X_train,X_test,Y_train,Y_test= train_test_split(X_scaled2,Y, test_size=0.1,random_state=42)
# X_train, Y_train=SMOTE(random_state=1).fit_resample(X_train,Y_train)

dec_tree_best_depth_impt_feat= tree.DecisionTreeClassifier(criterion='entropy',max_depth=6)
dec_tree_best_depth_impt_feat.fit(X_train,Y_train)
Y_pred_impt_feat=dec_tree_best_depth_impt_feat.predict(X_test)
depth={'max_depth': [2,6,10,12,25,28,30,36]}
grid_search=GridSearchCV(estimator=dec_tree_best_depth_impt_feat,param_grid=depth,scoring='precision',cv=5)
grid_search.fit(X_train,Y_train)
best_depth= grid_search.best_params_
print(best_depth)

dec_tree_best_depth_impt_feat= tree.DecisionTreeClassifier(criterion='entropy',max_depth=2)
dec_tree_best_depth_impt_feat.fit(X_train,Y_train)
Y_pred_impt_feat=dec_tree_best_depth_impt_feat.predict(X_test)

Accuracy= metrics.accuracy_score(Y_test,Y_pred)
Recall= metrics.recall_score(Y_test,Y_pred)
Precision= metrics.precision_score(Y_test,Y_pred)
print('Accuracy:', Accuracy)
print('Recall:', Recall)
print('Precision:', Precision)
conf_mat= metrics.confusion_matrix(Y_test,Y_pred)
print(conf_mat)

"""#random forest"""

# from sklearn import ensemble
# RF= ensemble.RandomForestClassifier(n_estimators=50,criterion='entropy',max_features=None,random_state=1)
# RF.fit(X_train,Y_train) # fitting random #forest to training data
# Y_pred=RF.predict(X_test)

# # Evaluating the random forest
# from sklearn import metrics
# Accuracy= metrics.accuracy_score(Y_test,Y_pred) # Calculating the accuracy of the model build
# Recall= metrics.recall_score(Y_test,Y_pred) # Calculating the recall of the model build
# Precision= metrics.precision_score(Y_test,Y_pred) # Calculating the precision of the model build
# print('Accuracy:', Accuracy)
# print('Recall:', Recall)
# print('Precision:', Precision)

# from sklearn.model_selection import GridSearchCV

# # Selecting the optimal n_estimators parameter
# RF1=ensemble.RandomForestClassifier(n_estimators=50,criterion='entropy',max_features=None,random_state=1)
# ntrees={'n_estimators': [100,150,180,220,250,300,320,350]} # Grid of n_estimators parameter length = 8

# grid_search=GridSearchCV(estimator=RF1,param_grid=ntrees,scoring='precision',cv=5)

# grid_search.fit(X_train,Y_train) # Fitting decision tree classifier to the training set
# best_nestimator= grid_search.best_params_ # Selecting the optimal parameter max_depth
# print(best_nestimator)

# # Refit the random forest on the optimal n_estimators parameter
# RF2=ensemble.RandomForestClassifier(n_estimators=100,criterion='entropy',max_features=None,random_state=1)
# RF2.fit(X_train,Y_train) # fitting random forest to training data
# Y_pred=RF2.predict(X_test)

# # Evaluating the refitted random forest with the optimal n_estimator parameter
# from sklearn import metrics
# Accuracy= metrics.accuracy_score(Y_test,Y_pred) # Calculating the accuracy of the model build
# Recall= metrics.recall_score(Y_test,Y_pred) # Calculating the recall of the model build
# Precision= metrics.precision_score(Y_test,Y_pred) # Calculating the precision of the model build
# print('Accuracy:', Accuracy)
# print('Recall:', Recall)
# print('Precision:', Precision)

#new
# from sklearn.tree import plot_tree
# import matplotlib.pyplot as plt

# plt.figure(figsize=(100,100))
# plot_tree(dec_tree,filled=True,feature_names=X.columns,class_names=['No','Yes'])
# plt.title('decision tree visualization')
# plt.show()

#random forest
# from sklearn import metrics
# from sklearn.ensemble import RandomForestClassifier
# from sklearn.metrics import accuracy_score,recall_score,precision_score
# rf_model=RandomForestClassifier(n_estimators=100,max_depth=2,random_state=42)
# rf_model.fit(X_train,Y_train)
# rf_predictions=rf_model.predict(X_test)
# rf_accuracy=accuracy_score(Y_test,rf_predictions)
# rf_recall=recall_score(Y_test,rf_predictions)
# rf_precision=precision_score(Y_test,rf_predictions)
# print('Accuracy:',rf_accuracy)
# print('Recall:',rf_recall)
# print('Precision:',rf_precision)
# # Y_pred=rf_model.predict(X_test)

#logistic regression
# from sklearn.linear_model import LogisticRegression
# log_reg=LogisticRegression(random_state=42,max_iter=1000)
# log_reg.fit(X_train,Y_train)
# log_reg_predictions=log_reg.predict(X_test)
# log_reg_accuracy=accuracy_score(Y_test,log_reg_predictions)
# log_reg_recall=recall_score(Y_test,log_reg_predictions)
# log_reg_precision=precision_score(Y_test,log_reg_predictions)
# print('Accuracy:',log_reg_accuracy)
# print('Recall:',log_reg_recall)
# print('Precision:',log_reg_precision)

# new_data_predictions=rf_model.predict(X_test)



# data['Gender']=data['Gender'].map({'Male':1,'Female':0})
# data['self_employed']=data['self_employed'].map({'Yes':1,'No':0})
# data['family_history']=data['family_history'].map({'Yes':1,'No':0})
# data['treatment']=data['treatment'].map({'Yes':1,'No':0})
# data['Growing_Stress']=data['Growing_Stress'].map({'Yes':1,'No':0})
# data['Coping_Struggles']=data['Coping_Struggles'].map({'Yes':1,'No':0})
# data['care_options']=data['care_options'].map({'Yes':1,'No':0,'Not sure':-1})

# data2=get_dummies(data,['	Timestamp','Country','Occupation','Days_Indoors','Changes_Habits','Mental_Health_History','Mood_Swings','Work_Interest','Social_Weakness','mental_health_interview'])

# imputer = SimpleImputer(strategy='mean')
# X_scaled = imputer.fit_transform(X_scaled)

# X_scaled=StandardScaler().fit_transform(X)

# X_train, X_test, Y_train, Y_test = train_test_split(X_scaled, Y, test_size = 0.3, random_state = 1)

# X_train, Y_train =SMOTE(random_state=1).fit_resample(X_train, Y_train)

# dec_tree= tree.DecisionTreeClassifier(criterion='entropy',max_depth=5)

# dec_tree.fit(X_train,Y_train)

# Y_prob=dec_tree.predict_proba(X_test)
# Y_pred = np.argmax(Y_prob, axis=1)

# Accuracy= metrics.accuracy_score(Y_test,Y_pred)
# Recall= metrics.recall_score(Y_test,Y_pred,average='weighted')
# Precision= metrics.precision_score(Y_test,Y_pred,average='weighted')
# print('Accuracy:', Accuracy)
# print('Recall:', Recall)
# print('Precision:', Precision)